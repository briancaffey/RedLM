services:
  redlm-api:
    build: redlm
    container_name: redlm-api
    command:
      - "fastapi"
      - "dev"
      - "redlm/main.py"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
    ports:
      - "8080:8000"
    env_file:
      - redlm/.env
    environment:
      - LLM_NAME
      - VLM_INVOKE_URL
      - VLM_SERVICE_HOST
      - VLM_SERVICE_PORT
      - VLM_MODEL_NAME
      - NVIDIA_API_KEY
      - LLM_SERVICE_HOST
      - LLM_SERVICE_PORT
      - COMPLETION_MODEL
      - MODEL_NAME
    volumes:
      - ./redlm:/app/redlm
      - ./redlm/storage:/app/storage
  redlm-ui:
    build:
      context: ui
      dockerfile: Dockerfile
      target: build
    container_name: redlm-ui
    environment:
      - NODE_ENV=development
      - NUXT_PUBLIC_REDLM_API_BASE=http://localhost
    # env_file:
    #   - ui/.env
    volumes:
      - ./ui:/app  # Mount the current directory to /app in the container
      - /app/node_modules  # Avoid overwriting the node_modules inside the container
      # mount the json and image files from the backend into the ui container
      - ./redlm/data/book/:/app/public/book
      - ./redlm/data/paintings/:/app/public/img/paintings
    ports:
      - "3000:3000"  # Expose the app on port 3000
    command: yarn dev --host  # Run the Nuxt dev server, use --host to expose on 0.0.0.0

  nginx:
    build:
      context: ./services/nginx
      dockerfile: Dockerfile
    container_name: nginx-proxy
    ports:
      - "80:80"  # Expose port 80
    depends_on:
      - redlm-api
      - redlm-ui
