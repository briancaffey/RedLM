# Environment Variables for Configuring RedLM

## Copy this file as `.env` and configure the following variables

NVIDIA_API_KEY=nvapi-abc123

LLM_NAME=baichuan-inc/baichuan2-13b-chat

# set these if using an LLM service other than the NVIDIA Cloud API
# LLM_SERVICE_HOST=localhost
# LLM_SERVICE_PORT=8000

# completion model - defaults to 01-ai/Yi-1.5-9B
COMPLETION_MODEL=01-ai/Yi-1.5-9B

# Vision Langauge Model configuration

## NVIDIA Cloud API VLM settings
VLM_MODEL_NAME=meta/llama-3.2-90b-vision-instruct
VLM_INVOKE_URL=https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-90b-vision-instruct/chat/completions

## Local VLM service configuration (services/qwen2-vl)
VLM_SERVICE_HOST=192.168.1.123
VLM_SERVICE_PORT=8000

# Optional configuration

# Use Milvus as Vector DB, otherwise use in-memory VectorStoreIndex
# USE_EXTERNAL_VECTORDB=True
# VECTORDB_SERVICE_HOST=localhost
# VECTORDB_SERVICE_PORT=19530

# Use Langfuse for observability
# LANGFUSE_SERVICE_HOST=localhost
# LANGFUSE_SERVICE_PORT=3030
# LANGFUSE_SECRET_KEY=sk-lf-...
# LANGFUSE_PUBLIC_KEY=pk-lf-...

TOKENIZERS_PARALLELISM=false